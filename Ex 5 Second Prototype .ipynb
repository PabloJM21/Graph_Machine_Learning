{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T20:43:27.727221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "import torch_scatter\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import to_networkx, subgraph\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# TASK 1 & 2: GINE Layer with Virtual Nodes (without node_encoder)\n",
    "class GINELayerWithVN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(GINELayerWithVN, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        self.edge_encoder = torch.nn.Linear(edge_dim, out_channels)\n",
    "        # Remove node_encoder from here\n",
    "        self.virtual_node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.edge_encoder.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "        for m in self.virtual_node_mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, vn_embed, batch):\n",
    "        # x is already encoded via node_encoder in the main model\n",
    "        x = x.float()  # Ensure x is FloatTensor\n",
    "        edge_attr = edge_attr.float()  # Ensure edge_attr is FloatTensor\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Add virtual node embedding to node features\n",
    "        vn_expanded = vn_embed[batch]\n",
    "        x = x + vn_expanded\n",
    "\n",
    "        # Message Passing\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        # Update node embeddings\n",
    "        out = self.mlp(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # Compute messages\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "# TASK 4: Laplacian Positional Encodings (LapPE)\n",
    "def compute_laplace_pe(data, num_eigenvec=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    num_nodes = A.shape[0]\n",
    "    D = np.diag(np.array(A.sum(axis=1)).flatten())\n",
    "    L = D - A.todense()\n",
    "    L = torch.tensor(L, dtype=torch.float, device=device)\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(L)\n",
    "    except RuntimeError:\n",
    "        eigenvalues, eigenvectors = torch.symeig(L, eigenvectors=True)\n",
    "    available_eigenvec = eigenvectors.shape[1] - 1\n",
    "    actual_num_eigenvec = min(num_eigenvec, available_eigenvec)\n",
    "    eigenvectors = eigenvectors[:, 1:1 + actual_num_eigenvec]\n",
    "    if actual_num_eigenvec < num_eigenvec:\n",
    "        pad_size = num_eigenvec - actual_num_eigenvec\n",
    "        padding = torch.zeros(eigenvectors.shape[0], pad_size, device=device)\n",
    "        eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "    return eigenvectors  # Shape: (num_nodes, num_eigenvec)\n",
    "\n",
    "# TASK 4: Random Walk Structural Embeddings (RWSE)\n",
    "def compute_rwse(data, walk_length=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    A = A.todense()\n",
    "    num_nodes = A.shape[0]\n",
    "    A = torch.tensor(A, dtype=torch.float, device=device)\n",
    "    rw_features = []\n",
    "    A_power = A.clone()\n",
    "    for _ in range(walk_length):\n",
    "        diag = torch.diagonal(A_power)\n",
    "        rw_features.append(diag)\n",
    "        A_power = torch.matmul(A_power, A)\n",
    "    rwse = torch.stack(rw_features, dim=1)  # (num_nodes, walk_length)\n",
    "    return rwse  # Shape: (num_nodes, walk_length)\n",
    "\n",
    "# TASK 4: SignNet to ensure sign invariance\n",
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.phi(x) + self.phi(-x)\n",
    "\n",
    "# Graph Transformer Layer\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, dropout=0.1):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=in_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(out_dim, in_dim)\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (sequence_length, batch_size, embed_dim)\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        linear_output = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = x + linear_output\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Updated GNN Model with Virtual Node, GINE Layers, and Graph Transformer\n",
    "class GNNWithVirtualNodeAndGINEAndTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, edge_attr_dim, num_layers=5, lap_pe_dim=10, rwse_dim=10):\n",
    "        super(GNNWithVirtualNodeAndGINEAndTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Node Encoder (moved outside GINE layers)\n",
    "        self.node_encoder = nn.Linear(in_features, hidden_features)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINELayerWithVN(\n",
    "                in_channels=hidden_features,\n",
    "                out_channels=hidden_features,\n",
    "                edge_dim=edge_attr_dim\n",
    "            ))\n",
    "\n",
    "        self.virtual_node_embedding = torch.nn.Embedding(1, hidden_features)\n",
    "        torch.nn.init.constant_(self.virtual_node_embedding.weight.data, 0)\n",
    "\n",
    "        self.mlp_virtual_node = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Positional Encodings\n",
    "        self.lap_pe_dim = lap_pe_dim\n",
    "        self.rwse_dim = rwse_dim\n",
    "        self.lap_pe_linear = nn.Linear(hidden_features, hidden_features)\n",
    "        self.rwse_linear = nn.Linear(rwse_dim, hidden_features)\n",
    "        self.signnet = SignNet(lap_pe_dim, hidden_features)\n",
    "\n",
    "        # Graph Transformer\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(hidden_features, hidden_features) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, data):\n",
    "        # Apply node_encoder first\n",
    "        x = self.node_encoder(x)  # [num_nodes, hidden_features]\n",
    "        # Initialize positional encodings tensor\n",
    "        pos_enc = torch.zeros_like(x).to(device)  # [num_nodes, hidden_features]\n",
    "\n",
    "        # Iterate over each graph in the batch\n",
    "        num_graphs = batch.max().item() + 1\n",
    "        for graph_id in range(num_graphs):\n",
    "            mask = (batch == graph_id)\n",
    "            num_nodes_graph = mask.sum().item()\n",
    "\n",
    "            # Extract node indices for the current graph\n",
    "            node_idx = torch.where(batch == graph_id)[0]\n",
    "\n",
    "            # Extract subgraph using pyg.utils.subgraph\n",
    "            sub_edge_index, sub_edge_attr = pyg.utils.subgraph(\n",
    "                node_idx,\n",
    "                edge_index,\n",
    "                edge_attr,\n",
    "                relabel_nodes=True,\n",
    "                num_nodes=x.size(0)\n",
    "            )\n",
    "\n",
    "            # Create sub_data\n",
    "            sub_data = pyg.data.Data(\n",
    "                x=x[node_idx],\n",
    "                edge_index=sub_edge_index,\n",
    "                edge_attr=sub_edge_attr\n",
    "            )\n",
    "\n",
    "            # Compute Positional Encodings for the sub-graph\n",
    "            # Compute Positional Encodings for the sub-graph\n",
    "            lap_pe = compute_laplace_pe(sub_data, num_eigenvec=self.lap_pe_dim)\n",
    "            rwse = compute_rwse(sub_data, walk_length=self.rwse_dim)\n",
    "            # Rest of the method remains unchanged\n",
    "\n",
    "            # Apply SignNet to LapPE\n",
    "            lap_pe = self.signnet(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Linear transformation\n",
    "            lap_pe = self.lap_pe_linear(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "            rwse = self.rwse_linear(rwse)        # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Combine positional encodings\n",
    "            graph_pos_enc = lap_pe + rwse  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "            # Assign to pos_enc\n",
    "            pos_enc[node_idx] = graph_pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "        # Add positional encodings to node features\n",
    "        x = x + pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "        # Initialize virtual node embedding\n",
    "        batch_size = num_graphs\n",
    "        vn_embed = self.virtual_node_embedding.weight.repeat(batch_size, 1)  # [batch_size, hidden_features]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr, vn_embed, batch)  # [num_nodes, hidden_features]\n",
    "            x = F.relu(x)\n",
    "\n",
    "            # Update virtual node embedding\n",
    "            vn_aggr = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "            vn_embed = vn_embed + self.mlp_virtual_node(vn_aggr)  # [batch_size, hidden_features]\n",
    "\n",
    "        # Prepare for Graph Transformer\n",
    "        # Group node features by graph and pad\n",
    "        x_padded, mask = pyg.utils.to_dense_batch(x, batch)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "\n",
    "        # Transpose to match expected input of Transformer\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [max_num_nodes, batch_size, hidden_features]\n",
    "\n",
    "        # Apply Transformer layers\n",
    "        for transformer in self.transformer_layers:\n",
    "            x_padded = transformer(x_padded)\n",
    "\n",
    "        # Transpose back\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "\n",
    "        # Flatten x_padded back to x\n",
    "        x = x_padded[mask]  # x: [num_nodes, hidden_features]\n",
    "\n",
    "        # Apply global mean pooling\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "        x = self.fc(x)  # [batch_size, out_features]\n",
    "        return x\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        batch = batch.to(device)\n",
    "        batch.x = batch.x.float()  # Convert node features to float\n",
    "        batch.edge_attr = batch.edge_attr.float()  # Convert edge attributes to float\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, batch)\n",
    "        loss = loss_fn(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(loader)\n",
    "    return average_loss\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            batch = batch.to(device)\n",
    "            batch.x = batch.x.float()  # Convert node features to float\n",
    "            batch.edge_attr = batch.edge_attr.float()  # Convert edge attributes to float\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, batch)\n",
    "            y_pred.append(out.cpu())\n",
    "            y_true.append(batch.y.cpu())\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "    # Compute per-class AP\n",
    "    ap_per_class = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        try:\n",
    "            ap = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "        except ValueError:\n",
    "            ap = 0.0  # Handle cases where a class has no positive samples\n",
    "        ap_per_class.append(ap)\n",
    "    mean_ap = np.mean(ap_per_class)\n",
    "    return mean_ap\n",
    "\n",
    "def plot_results(epochs, train_losses, val_aps, learning_rates=None):\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Training_Loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Validation AP Score\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, val_aps, label='Validation AP Score', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Precision Score')\n",
    "    plt.title('Validation AP Score over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Validation_AP_Score.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Learning Rate if provided\n",
    "    if learning_rates is not None:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(epochs_range, learning_rates, label='Learning Rate', color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('Learning_Rate.png')\n",
    "        plt.show()\n",
    "\n",
    "def main(epochs=100, lr=0.001, hidden_features=256):\n",
    "    # Compute edge_attr_dim and num_tasks from the dataset\n",
    "    edge_attr_dim = dataset[0].edge_attr.shape[1]\n",
    "    num_tasks = dataset[0].y.shape[-1]\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = GNNWithVirtualNodeAndGINEAndTransformer(\n",
    "        in_features=dataset.num_node_features,\n",
    "        hidden_features=hidden_features,\n",
    "        out_features=num_tasks,\n",
    "        edge_attr_dim=edge_attr_dim,\n",
    "        num_layers=5,  # Increased depth as per the paper's suggestion\n",
    "        lap_pe_dim=10,\n",
    "        rwse_dim=10\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.9, patience=10)\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Lists to store losses and AP scores\n",
    "    train_losses = []\n",
    "    val_aps = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "        val_ap = evaluate(model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_aps.append(val_ap)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Validation AP Score: {val_ap:.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "        scheduler.step(val_ap)\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_ap = evaluate(model, test_loader)\n",
    "    print(f\"Test AP Score: {test_ap:.4f}\")\n",
    "\n",
    "    # Plotting the results\n",
    "    plot_results(epochs, train_losses, val_aps, learning_rates)\n",
    "\n",
    "# Task 4: Draw the molecule represented by peptides_train[0]\n",
    "def draw_molecule(data, def_col=0):\n",
    "    G = pyg.utils.to_networkx(data, to_undirected=True)\n",
    "    node_features = data.x.numpy()\n",
    "    edge_index = data.edge_index.numpy()\n",
    "    edge_attr = data.edge_attr.numpy()\n",
    "    bond_types = edge_attr[:, 0].astype(int)\n",
    "    atom_types = None\n",
    "    atom_type_indices = None\n",
    "    for i, (u, v) in enumerate(zip(edge_index[0], edge_index[1])):\n",
    "        G.edges[u, v]['bond_type'] = bond_types[i]\n",
    "    if def_col == 0:\n",
    "        atom_types = {\n",
    "            5: 'C',\n",
    "            6: 'N',\n",
    "            7: 'O',\n",
    "        }\n",
    "        atom_type_indices = node_features[:, def_col].astype(int)\n",
    "    elif def_col == 2:\n",
    "        atom_types = {4: 'C', 3: 'O', 1: 'N'}\n",
    "        atom_type_indices = node_features[:, def_col].astype(int)\n",
    "    elif def_col == 4:\n",
    "        atom_types = {1: 'C', 0: 'O', 2: 'N'}\n",
    "        atom_type_indices = node_features[:, def_col].astype(int)\n",
    "    bond_color_mapping = {\n",
    "        0: 'black',\n",
    "        1: 'blue',\n",
    "        3: 'red',\n",
    "    }\n",
    "    edges = list(G.edges())\n",
    "    edge_colors = []\n",
    "    for u, v in edges:\n",
    "        bond_type = G.edges[u, v]['bond_type']\n",
    "        color = bond_color_mapping.get(bond_type, 'green')\n",
    "        edge_colors.append(color)\n",
    "    labels = {i: atom_types.get(atom_type_indices[i], 'X') for i in range(atom_type_indices.shape[0])}\n",
    "    size=12\n",
    "    plt.figure(figsize=(size, size))\n",
    "    pos = nx.kamada_kawai_layout(G, scale=5)\n",
    "    nx.draw(\n",
    "        G, pos,\n",
    "        with_labels=False,\n",
    "        node_size=50,\n",
    "        node_color='lightblue',\n",
    "        edgelist=edges,\n",
    "        edge_color=edge_colors,\n",
    "        width=1.5\n",
    "    )\n",
    "    nx.draw_networkx_labels(\n",
    "        G, pos,\n",
    "        labels=labels,\n",
    "        font_size=6,\n",
    "        font_weight='bold'\n",
    "    )\n",
    "    plt.title('Molecule Visualization of peptides_train[0]')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('Molecule_Visualization.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset and create data loaders\n",
    "    # Replace LRGBDataset with an appropriate dataset loader if needed\n",
    "    # Here, I'll assume you're using a custom dataset similar to TUDataset\n",
    "\n",
    "    try:\n",
    "        dataset = pyg.datasets.LRGBDataset(root='dataset/peptides-func', name=\"Peptides-func\")\n",
    "    except AttributeError:\n",
    "        # If LRGBDataset is not available, use a placeholder\n",
    "        # Replace this with the actual dataset loader you're using\n",
    "        print(\"LRGBDataset not found. Please replace with the actual dataset loader.\")\n",
    "        dataset = TUDataset(root='dataset/Mutagenicity', name='Mutagenicity')\n",
    "\n",
    "    # Check if dataset has splits; if not, create them manually\n",
    "    if hasattr(dataset, 'train_val_test_idx'):\n",
    "        peptides_train = dataset[dataset.train_val_test_idx['train']]\n",
    "        peptides_val = dataset[dataset.train_val_test_idx['val']]\n",
    "        peptides_test = dataset[dataset.train_val_test_idx['test']]\n",
    "    else:\n",
    "        # Create train, val, test splits manually\n",
    "        num_train = int(0.8 * len(dataset))\n",
    "        num_val = int(0.1 * len(dataset))\n",
    "        num_test = len(dataset) - num_train - num_val\n",
    "        peptides_train, peptides_val, peptides_test = torch.utils.data.random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = pyg.loader.DataLoader(peptides_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = pyg.loader.DataLoader(peptides_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = pyg.loader.DataLoader(peptides_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Check number of classes and label distribution\n",
    "    if hasattr(dataset, 'num_tasks'):\n",
    "        num_classes = dataset.num_tasks\n",
    "    elif hasattr(dataset, 'num_classes'):\n",
    "        num_classes = dataset.num_classes\n",
    "    else:\n",
    "        # Assume binary classification if not specified\n",
    "        num_classes = 1\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    all_labels = np.concatenate([data.y.numpy() for data in dataset], axis=0)\n",
    "    label_distribution = np.mean(all_labels, axis=0)\n",
    "    print(f\"Label distribution: {label_distribution}\")\n",
    "\n",
    "    # Run the main training loop\n",
    "    main(epochs=300, lr=0.001, hidden_features=128)\n",
    "\n",
    "    # Draw the molecule for Task 4\n",
    "    if len(peptides_train) > 0:\n",
    "        draw_molecule(peptides_train[0])\n",
    "    else:\n",
    "        print(\"Training set is empty. Cannot draw a molecule.\")\n"
   ],
   "id": "818a486a6ed6da62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes: 10\n",
      "Label distribution: [0.08884393 0.03540881 0.06419571 0.06226432 0.6272418  0.19755358\n",
      " 0.10687023 0.18412581 0.01995769 0.2598179 ]\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 154/272 [00:42<00:30,  3.87it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98992f60c0c48494"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
