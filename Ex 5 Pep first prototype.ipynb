{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T14:27:25.825048Z"
    }
   },
   "source": [
    "# graph_transformer_ogbg_molpcba_fixed.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "import torch_geometric.transforms as T\n",
    "import scipy\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ===============================\n",
    "# 1. Configure Logging\n",
    "# ===============================\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger('GraphTransformerLogger')\n",
    "logger.setLevel(logging.INFO)  # Set to INFO during normal training\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler('debug.log')\n",
    "file_handler.setLevel(logging.DEBUG)  # Capture all logs in the file\n",
    "\n",
    "# Optional: Create console handler if you still want some output on the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)  # Display INFO and above on console\n",
    "\n",
    "# Create formatter and add it to handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# ===============================\n",
    "# 2. Device Configuration\n",
    "# ===============================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f'Using device: {device}')\n",
    "\n",
    "# Enable CUDNN benchmarking for optimized performance\n",
    "torch.backends.cudnn.benchmark = True\n",
    "logger.info(\"Enabled CUDNN benchmarking for optimized performance.\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Define Data Transformation Classes\n",
    "# ===============================\n",
    "\n",
    "# Define BondAttributeMapper with vectorized mapping\n",
    "class BondAttributeMapper(object):\n",
    "    def __init__(self):\n",
    "        # Define maximum allowed values for each bond feature\n",
    "        self.max_bond_type = 4\n",
    "        self.max_bond_stereo = 2\n",
    "        self.max_bond_conj = 2\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if data.edge_attr is not None:\n",
    "            # Ensure bond attributes are of type long\n",
    "            data.edge_attr = data.edge_attr.long()\n",
    "\n",
    "            # Clamp bond_type to [0, max_bond_type]\n",
    "            data.edge_attr[:, 0] = torch.clamp(data.edge_attr[:, 0], max=self.max_bond_type)\n",
    "\n",
    "            # Clamp bond_stereo to [0, max_bond_stereo]\n",
    "            data.edge_attr[:, 1] = torch.clamp(data.edge_attr[:, 1], max=self.max_bond_stereo)\n",
    "\n",
    "            # Clamp bond_conj to [0, max_bond_conj]\n",
    "            data.edge_attr[:, 2] = torch.clamp(data.edge_attr[:, 2], max=self.max_bond_conj)\n",
    "        else:\n",
    "            # Assign default bond attributes if missing\n",
    "            num_edges = data.edge_index.size(1)\n",
    "            data.edge_attr = torch.zeros((num_edges, 3), dtype=torch.long).to(device)\n",
    "            logger.debug(f\"Assigned default bond attributes for graph with {num_edges} edges.\")\n",
    "        return data\n",
    "\n",
    "# Define Laplacian Positional Encoding\n",
    "class LaplacianPositionalEncoding(object):\n",
    "    def __init__(self, pe_dim):\n",
    "        self.pe_dim = pe_dim\n",
    "\n",
    "    def __call__(self, data):\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        # Convert edge_index to adjacency matrix\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsc()\n",
    "\n",
    "        # Compute the Laplacian\n",
    "        laplacian = scipy.sparse.csgraph.laplacian(adj, normed=True)\n",
    "\n",
    "        # Compute the eigenvalues and eigenvectors\n",
    "        try:\n",
    "            eigenvalues, eigenvectors = scipy.linalg.eigh(laplacian.toarray())\n",
    "            logger.debug(f\"Computed Laplacian eigenvectors for graph with {num_nodes} nodes.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing eigenvectors for graph with {num_nodes} nodes: {e}\")\n",
    "            eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())\n",
    "\n",
    "        # Handle small graphs where num_nodes -1 < pe_dim\n",
    "        actual_pe_dim = min(self.pe_dim, num_nodes - 1)\n",
    "        if actual_pe_dim <= 0:\n",
    "            pe = torch.zeros((num_nodes, self.pe_dim)).float().to(device)\n",
    "            logger.debug(f\"Graph with {num_nodes} nodes: Using zero-padding for LapPE.\")\n",
    "        else:\n",
    "            pe = torch.from_numpy(eigenvectors[:, 1:1 + actual_pe_dim]).float().to(device)\n",
    "            if actual_pe_dim < self.pe_dim:\n",
    "                padding = torch.zeros((num_nodes, self.pe_dim - actual_pe_dim)).float().to(device)\n",
    "                pe = torch.cat([pe, padding], dim=1)\n",
    "                logger.debug(f\"Graph with {num_nodes} nodes: Padding LapPE from {actual_pe_dim} to {self.pe_dim} dimensions.\")\n",
    "        data.lap_pe = pe  # Shape: [num_nodes, pe_dim]\n",
    "        return data\n",
    "\n",
    "# Define Random Walk Structural Encoding\n",
    "class RandomWalkStructuralEncoding(object):\n",
    "    def __init__(self, walk_length):\n",
    "        self.walk_length = walk_length\n",
    "\n",
    "    def __call__(self, data):\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsc()\n",
    "\n",
    "        diag = []\n",
    "        A = adj.copy()\n",
    "\n",
    "        for k in range(1, self.walk_length + 1):\n",
    "            A_power = A ** k\n",
    "            diag_k = A_power.diagonal()\n",
    "            diag.append(diag_k)\n",
    "\n",
    "        diag = np.stack(diag, axis=1)  # Shape: [num_nodes, walk_length]\n",
    "        rwse = torch.from_numpy(diag).float().to(device)\n",
    "        desired_walk_length = self.walk_length\n",
    "        if rwse.shape[1] < desired_walk_length:\n",
    "            padding = torch.zeros((num_nodes, desired_walk_length - rwse.shape[1])).float().to(device)\n",
    "            rwse = torch.cat([rwse, padding], dim=1)\n",
    "            logger.debug(f\"Graph with {num_nodes} nodes: Padding RWSE from {rwse.shape[1]} to {desired_walk_length} dimensions.\")\n",
    "        data.rwse = rwse  # Shape: [num_nodes, walk_length]\n",
    "        return data\n",
    "\n",
    "# ===============================\n",
    "# 4. Define Model Components\n",
    "# ===============================\n",
    "\n",
    "# Define CustomBondEncoder\n",
    "class CustomBondEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim=64, num_bond_types=5, num_bond_stereo=3, num_bond_conj=3):\n",
    "        super().__init__()\n",
    "        self.bond_embedding_list = nn.ModuleList([\n",
    "            nn.Embedding(num_bond_types, emb_dim),    # bond type\n",
    "            nn.Embedding(num_bond_stereo, emb_dim),   # bond stereo\n",
    "            nn.Embedding(num_bond_conj, emb_dim)      # bond conjugation\n",
    "        ])\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        bond_embedding = 0\n",
    "        for i in range(edge_attr.shape[1]):\n",
    "            bond_embedding += self.bond_embedding_list[i](edge_attr[:, i])\n",
    "        return bond_embedding  # Shape: [num_edges, emb_dim]\n",
    "\n",
    "# Define SignNet to ensure sign invariance\n",
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, pe):\n",
    "        pe_pos = self.mlp(pe)\n",
    "        pe_neg = self.mlp(-pe)\n",
    "        pe_sign_invariant = pe_pos + pe_neg  # Ensures f(ν) = f(-ν)\n",
    "        return pe_sign_invariant  # Shape: [num_nodes, out_dim]\n",
    "\n",
    "# Define GraphTransformer using TransformerConv\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, num_layers, num_heads, dropout=0.1,\n",
    "                 num_bond_types=5, num_bond_stereo=3, num_bond_conj=3):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_encoder = CustomBondEncoder(emb_dim=hidden_dim,\n",
    "                                              num_bond_types=num_bond_types,\n",
    "                                              num_bond_stereo=num_bond_stereo,\n",
    "                                              num_bond_conj=num_bond_conj)\n",
    "        self.sign_net = SignNet(in_dim=10, hidden_dim=hidden_dim, out_dim=hidden_dim)\n",
    "        self.rwse_mlp = nn.Sequential(\n",
    "            nn.Linear(10, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.transformer_convs = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            conv = TransformerConv(hidden_dim, hidden_dim // num_heads,\n",
    "                                   heads=num_heads, dropout=dropout, edge_dim=hidden_dim)\n",
    "            self.transformer_convs.append(conv)\n",
    "            logger.debug(f\"Initialized TransformerConv layer {layer + 1}/{num_layers} with out_channels={hidden_dim // num_heads}.\")\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "        logger.info(\"GraphTransformer initialization complete.\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.to(device))  # Shape: [num_nodes, hidden_dim]\n",
    "\n",
    "        # Incorporate positional encodings\n",
    "        pe = data.lap_pe.to(device)  # Shape: [num_nodes, pe_dim]\n",
    "        pe = self.sign_net(pe)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + pe  # Incorporate positional encodings\n",
    "\n",
    "        # Incorporate structural encodings\n",
    "        rwse = data.rwse.to(device)  # Shape: [num_nodes, walk_length]\n",
    "        rwse = self.rwse_mlp(rwse)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + rwse  # Incorporate structural encodings\n",
    "\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        edge_attr = data.edge_attr.to(device)\n",
    "\n",
    "        # Debug: Check edge_attr dimensions and values\n",
    "        logger.debug(f\"GraphTransformer - Edge_attr shape: {edge_attr.shape}\")\n",
    "        logger.debug(f\"GraphTransformer - Edge_attr max per feature: {edge_attr.max(dim=0).values}\")\n",
    "\n",
    "        edge_attr = self.bond_encoder(edge_attr)  # Shape: [num_edges, emb_dim]\n",
    "        logger.debug(f\"GraphTransformer - Bond Embedding shape: {edge_attr.shape}\")\n",
    "\n",
    "        for conv in self.transformer_convs:\n",
    "            x = conv(x, edge_index, edge_attr)  # Shape: [num_nodes, hidden_dim]\n",
    "            x = F.relu(x)\n",
    "            logger.debug(f\"GraphTransformer - After TransformerConv: x shape: {x.shape}\")\n",
    "\n",
    "        x = global_mean_pool(x, data.batch)  # Shape: [batch_size, hidden_dim]\n",
    "        out = self.fc_out(x)  # Shape: [batch_size, out_dim]\n",
    "        return out\n",
    "\n",
    "# Define PureTransformer Model\n",
    "class PureTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, num_layers, num_heads, dropout=0.1,\n",
    "                 num_bond_types=5, num_bond_stereo=3, num_bond_conj=3):\n",
    "        super(PureTransformer, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_encoder = CustomBondEncoder(emb_dim=hidden_dim,\n",
    "                                              num_bond_types=num_bond_types,\n",
    "                                              num_bond_stereo=num_bond_stereo,\n",
    "                                              num_bond_conj=num_bond_conj)\n",
    "        self.sign_net = SignNet(in_dim=10, hidden_dim=hidden_dim, out_dim=hidden_dim)\n",
    "        self.rwse_mlp = nn.Sequential(\n",
    "            nn.Linear(10, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "        logger.info(\"PureTransformer initialization complete.\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.to(device))  # Shape: [num_nodes, hidden_dim]\n",
    "\n",
    "        # Incorporate positional encodings\n",
    "        pe = data.lap_pe.to(device)  # Shape: [num_nodes, pe_dim]\n",
    "        pe = self.sign_net(pe)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + pe  # Incorporate positional encodings\n",
    "\n",
    "        # Incorporate structural encodings\n",
    "        rwse = data.rwse.to(device)  # Shape: [num_nodes, walk_length]\n",
    "        rwse = self.rwse_mlp(rwse)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + rwse  # Incorporate structural encodings\n",
    "\n",
    "        # Transformers expect input of shape (sequence_length, batch_size, embedding_dim)\n",
    "        # In PyG batching, nodes from different graphs are concatenated, so batch_size=1\n",
    "        x = x.unsqueeze(1)  # Shape: [num_nodes, 1, hidden_dim]\n",
    "\n",
    "        # Apply Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: [num_nodes, 1, hidden_dim]\n",
    "        x = x.squeeze(1)  # Shape: [num_nodes, hidden_dim]\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, data.batch)  # Shape: [batch_size, hidden_dim]\n",
    "        out = self.fc_out(x)  # Shape: [batch_size, out_dim]\n",
    "        return out\n",
    "\n",
    "# Define GCN model\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, num_layers,\n",
    "                 num_bond_types=5, num_bond_stereo=3, num_bond_conj=3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_encoder = CustomBondEncoder(emb_dim=hidden_dim,\n",
    "                                              num_bond_types=num_bond_types,\n",
    "                                              num_bond_stereo=num_bond_stereo,\n",
    "                                              num_bond_conj=num_bond_conj)\n",
    "        self.sign_net = SignNet(in_dim=10, hidden_dim=hidden_dim, out_dim=hidden_dim)\n",
    "        self.rwse_mlp = nn.Sequential(\n",
    "            nn.Linear(10, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        logger.debug(f\"Initialized GCNConv layer 1/{num_layers}.\")\n",
    "        for layer in range(1, num_layers):\n",
    "            conv = GCNConv(hidden_dim, hidden_dim)\n",
    "            self.convs.append(conv)\n",
    "            logger.debug(f\"Initialized GCNConv layer {layer + 1}/{num_layers}.\")\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "        logger.info(\"GCN initialization complete.\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.to(device))  # Shape: [num_nodes, hidden_dim]\n",
    "\n",
    "        # Incorporate positional encodings\n",
    "        pe = data.lap_pe.to(device)  # Shape: [num_nodes, pe_dim]\n",
    "        pe = self.sign_net(pe)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + pe  # Incorporate positional encodings\n",
    "\n",
    "        # Incorporate structural encodings\n",
    "        rwse = data.rwse.to(device)  # Shape: [num_nodes, walk_length]\n",
    "        rwse = self.rwse_mlp(rwse)  # Shape: [num_nodes, hidden_dim]\n",
    "        x = x + rwse  # Incorporate structural encodings\n",
    "\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        edge_attr = data.edge_attr.to(device)\n",
    "\n",
    "        # Debug: Check edge_attr dimensions and values\n",
    "        logger.debug(f\"GCN - Edge_attr shape: {edge_attr.shape}\")\n",
    "        logger.debug(f\"GCN - Edge_attr max per feature: {edge_attr.max(dim=0).values}\")\n",
    "\n",
    "        edge_attr = self.bond_encoder(edge_attr)  # Shape: [num_edges, emb_dim]\n",
    "        logger.debug(f\"GCN - Bond Embedding shape: {edge_attr.shape}\")\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)  # Shape: [num_nodes, hidden_dim]\n",
    "            x = F.relu(x)\n",
    "            logger.debug(f\"GCN - After GCNConv: x shape: {x.shape}\")\n",
    "\n",
    "        x = global_mean_pool(x, data.batch)  # Shape: [batch_size, hidden_dim]\n",
    "        out = self.fc_out(x)  # Shape: [batch_size, out_dim]\n",
    "        return out\n",
    "\n",
    "# ===============================\n",
    "# 5. Prepare Dataset with Transforms\n",
    "# ===============================\n",
    "\n",
    "transform = Compose([\n",
    "    T.ToUndirected(),\n",
    "    BondAttributeMapper(),  # Use the corrected mapper\n",
    "    LaplacianPositionalEncoding(pe_dim=10),\n",
    "    RandomWalkStructuralEncoding(walk_length=10)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "dataset = PygGraphPropPredDataset(name='ogbg-molpcba', root='data/ogbg_molpcba', transform=transform)\n",
    "logger.info(f\"Number of graphs in the dataset: {len(dataset)}\")\n",
    "logger.info(f\"Number of tasks: {dataset.num_tasks}\")\n",
    "logger.info(f\"Example graph:\")\n",
    "logger.info(dataset[0])\n",
    "\n",
    "# Data splitting\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]\n",
    "valid_dataset = dataset[split_idx['valid']]\n",
    "test_dataset = dataset[split_idx['test']]\n",
    "\n",
    "logger.info(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "logger.info(f\"Number of validation graphs: {len(valid_dataset)}\")\n",
    "logger.info(f\"Number of test graphs: {len(test_dataset)}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Create DataLoaders\n",
    "# ===============================\n",
    "\n",
    "# Create DataLoaders with multiple workers and pin_memory for faster data transfer\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# 7. Inspect Bond Attributes in a Batch\n",
    "# ===============================\n",
    "\n",
    "def inspect_bond_attributes(loader):\n",
    "    for batch_idx, data in tqdm(enumerate(loader)):\n",
    "        print(\"x\")\n",
    "        if data.edge_attr is not None:\n",
    "            max_values = data.edge_attr.max(dim=0).values\n",
    "            logger.debug(f\"Batch {batch_idx} - Max Bond Attributes: {max_values}\")\n",
    "        else:\n",
    "            logger.debug(f\"Batch {batch_idx} - No Bond Attributes Found.\")\n",
    "        # Inspect only the first few batches\n",
    "        if batch_idx >= 5:\n",
    "            break\n",
    "\n",
    "logger.info(\"\\nInspecting Bond Attributes in Training Data:\")\n",
    "inspect_bond_attributes(train_loader)\n",
    "\n",
    "# ===============================\n",
    "# 8. Initialize Evaluator and Loss Function\n",
    "# ===============================\n",
    "\n",
    "evaluator = Evaluator(name='ogbg-molpcba')\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ===============================\n",
    "# 9. Define Training and Evaluation Functions\n",
    "# ===============================\n",
    "\n",
    "def train_model(model, loader, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\", leave=False)\n",
    "    for batch_idx, data in progress_bar:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            out = model(data)  # Shape: [batch_size, num_tasks]\n",
    "\n",
    "            # Mask out invalid targets (-1)\n",
    "            is_labeled = data.y != -1  # Shape: [batch_size, num_tasks]\n",
    "            if torch.sum(is_labeled).item() == 0:\n",
    "                logger.debug(f\"Batch {batch_idx}: No labeled data, skipping.\")\n",
    "                continue  # Skip if no labels are present\n",
    "\n",
    "            # Clamp labels to [0,1]\n",
    "            y_true = data.y.clone()\n",
    "            y_true[~is_labeled] = 0  # Set unlabeled to 0 (won't affect loss)\n",
    "            y_true = y_true.float()\n",
    "\n",
    "            # Debug: Print label statistics\n",
    "            if torch.any((y_true < 0) | (y_true > 1)):\n",
    "                logger.warning(f\"Batch {batch_idx}: Labels out of range [0, 1].\")\n",
    "                logger.warning(f\"Labels: {y_true}\")\n",
    "\n",
    "            # Compute loss only on labeled data\n",
    "            loss = criterion(out[is_labeled], y_true[is_labeled])\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        # Update progress bar description every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            progress_bar.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    logger.debug(f\"Average Training Loss: {average_loss:.4f}\")\n",
    "    return average_loss\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    progress_bar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for data in progress_bar:\n",
    "            data = data.to(device)\n",
    "            out = model(data)  # Shape: [batch_size, num_tasks]\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "    y_true = torch.cat(y_true, dim=0)  # Shape: [num_graphs, num_tasks]\n",
    "    y_pred = torch.cat(y_pred, dim=0)  # Shape: [num_graphs, num_tasks]\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    result = evaluator.eval(input_dict)\n",
    "    logger.debug(f\"Evaluation Result: {result}\")\n",
    "    return result[\"ap\"]  # Average Precision\n",
    "\n",
    "# ===============================\n",
    "# 10. Initialize Models and Optimizers\n",
    "# ===============================\n",
    "\n",
    "hidden_dim = 64\n",
    "out_dim = dataset.num_tasks  # 128 for ogbg-molpcba\n",
    "num_layers = 3\n",
    "num_heads = 4\n",
    "\n",
    "# Instantiate GraphTransformer and PureTransformer\n",
    "logger.info(\"\\nInitializing GraphTransformer:\")\n",
    "model_transformer_conv = GraphTransformer(hidden_dim=hidden_dim, out_dim=out_dim,\n",
    "                                          num_layers=num_layers, num_heads=num_heads).to(device)\n",
    "optimizer_transformer_conv = torch.optim.Adam(model_transformer_conv.parameters(), lr=0.001)\n",
    "\n",
    "logger.info(\"\\nInitializing PureTransformer:\")\n",
    "model_pure_transformer = PureTransformer(hidden_dim=hidden_dim, out_dim=out_dim,\n",
    "                                         num_layers=num_layers, num_heads=num_heads).to(device)\n",
    "optimizer_pure_transformer = torch.optim.Adam(model_pure_transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Instantiate GCN model\n",
    "logger.info(\"\\nInitializing GCN:\")\n",
    "model_gcn = GCN(hidden_dim=hidden_dim, out_dim=out_dim, num_layers=num_layers).to(device)\n",
    "optimizer_gcn = torch.optim.Adam(model_gcn.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize GradScaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "logger.info(\"Initialized GradScaler for mixed precision training.\")\n",
    "\n",
    "# ===============================\n",
    "# 11. Training Loop\n",
    "# ===============================\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training GraphTransformer with TransformerConv layers\n",
    "logger.info(\"\\nStarting Training for GraphTransformer:\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_model(model_transformer_conv, train_loader, optimizer_transformer_conv, scaler)\n",
    "    val_ap = evaluate_model(model_transformer_conv, valid_loader)\n",
    "    test_ap = evaluate_model(model_transformer_conv, test_loader)\n",
    "    logger.info(f'[TransformerConv] Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_ap:.4f}, Test AP: {test_ap:.4f}')\n",
    "\n",
    "# Training Pure Transformer Model\n",
    "logger.info(\"\\nStarting Training for PureTransformer:\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_model(model_pure_transformer, train_loader, optimizer_pure_transformer, scaler)\n",
    "    val_ap = evaluate_model(model_pure_transformer, valid_loader)\n",
    "    test_ap = evaluate_model(model_pure_transformer, test_loader)\n",
    "    logger.info(f'[PureTransformer] Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_ap:.4f}, Test AP: {test_ap:.4f}')\n",
    "\n",
    "# Training GCN Model\n",
    "logger.info(\"\\nStarting Training for GCN:\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_model(model_gcn, train_loader, optimizer_gcn, scaler)\n",
    "    val_ap = evaluate_model(model_gcn, valid_loader)\n",
    "    test_ap = evaluate_model(model_gcn, test_loader)\n",
    "    logger.info(f'[GCN] Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_ap:.4f}, Test AP: {test_ap:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 15:27:29,234 - INFO - Using device: cuda\n",
      "2024-11-25 15:27:29,234 - INFO - Enabled CUDNN benchmarking for optimized performance.\n",
      "2024-11-25 15:27:29,234 - INFO - Loading dataset...\n",
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\ogb\\graphproppred\\dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "2024-11-25 15:27:32,838 - INFO - Number of graphs in the dataset: 437929\n",
      "2024-11-25 15:27:32,854 - INFO - Number of tasks: 128\n",
      "2024-11-25 15:27:32,854 - INFO - Example graph:\n",
      "2024-11-25 15:27:33,016 - INFO - Data(edge_index=[2, 44], edge_attr=[44, 3], x=[20, 9], y=[1, 128], num_nodes=20, lap_pe=[20, 10], rwse=[20, 10])\n",
      "2024-11-25 15:27:33,110 - INFO - Number of training graphs: 350343\n",
      "2024-11-25 15:27:33,110 - INFO - Number of validation graphs: 43793\n",
      "2024-11-25 15:27:33,110 - INFO - Number of test graphs: 43793\n",
      "2024-11-25 15:27:33,110 - INFO - \n",
      "Inspecting Bond Attributes in Training Data:\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c6982a43a687f30a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e94c192b176b3571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dadfdf17d102657a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
