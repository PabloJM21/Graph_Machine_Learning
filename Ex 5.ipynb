{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.160115Z",
     "start_time": "2024-11-24T20:09:14.154075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy.linalg import eigh\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ],
   "id": "9b2dc62bf3e1f127",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Implement Laplacian Positional Encodings (LapPE)\n",
    " - Compute Laplacian eigenvectors using the graph Laplacian matrix `L = D - A`.\n",
    " - Use `eigh` for efficient eigendecomposition (optimized for Hermitian matrices).\n",
    " - Handle sign ambiguity in eigenvectors (random flipping or SignNet).\n",
    " - Ensure embeddings are robust to sign flips and can capture graph structure."
   ],
   "id": "1eef9a801a05f30c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.268118Z",
     "start_time": "2024-11-24T20:09:14.263046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_lap_pe(data, pe_dim=10, use_signnet=False):\n",
    "    \"\"\"\n",
    "    Compute Laplacian Positional Encodings (LapPE) for a given graph data.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): Graph data object.\n",
    "        pe_dim (int): Number of eigenvectors to compute.\n",
    "        use_signnet (bool): Whether to handle sign ambiguity using SignNet.\n",
    "\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): Graph data object with 'lap_pe' attribute added.\n",
    "    \"\"\"\n",
    "    num_nodes = data.num_nodes\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Convert to scipy sparse matrix\n",
    "    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).astype(float)\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    laplacian = csgraph.laplacian(adj, normed=False)\n",
    "\n",
    "    # For large sparse matrices, use eigsh\n",
    "    try:\n",
    "        # Compute the smallest k+1 eigenvalues and eigenvectors\n",
    "        eigvals, eigvecs = eigsh(laplacian, k=pe_dim+1, which='SM')\n",
    "        eigvecs = eigvecs[:, eigvals.argsort()]  # Sort eigenvectors\n",
    "    except RuntimeError:\n",
    "        # Fall back to dense computation for small graphs\n",
    "        laplacian_dense = laplacian.toarray()\n",
    "        eigvals, eigvecs = eigh(laplacian_dense)\n",
    "        eigvecs = eigvecs[:, eigvals.argsort()]\n",
    "\n",
    "    # Exclude the first eigenvector (corresponding to the smallest eigenvalue)\n",
    "    pe = eigvecs[:, 1:pe_dim+1]\n",
    "\n",
    "    # Handle sign ambiguity\n",
    "    if use_signnet:\n",
    "        # SignNet implementation (placeholder)\n",
    "        # You would implement SignNet as a neural network layer in your model\n",
    "        # For now, we'll assume the sign ambiguity is handled during model training\n",
    "        pass\n",
    "    else:\n",
    "        # Randomly flip signs during training (data augmentation)\n",
    "        # This helps the model to be robust to sign flips\n",
    "        sign_flip = np.random.choice([-1, 1], size=(pe.shape[1],))\n",
    "        pe = pe * sign_flip\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    pe_tensor = torch.from_numpy(pe).float()\n",
    "\n",
    "    # Add to data\n",
    "    data.lap_pe = pe_tensor\n",
    "\n",
    "    return data\n",
    "\n"
   ],
   "id": "4a82cb3904df2f7a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example",
   "id": "13867964134f10cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.297419Z",
     "start_time": "2024-11-24T20:09:14.276191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Create a simple graph\n",
    "# For this example, we'll create a graph with 5 nodes and some edges\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3, 0, 2],\n",
    "    [1, 2, 3, 4, 4, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create a Data object\n",
    "data = Data(edge_index=edge_index)\n",
    "\n",
    "# Set the number of nodes (important if some nodes have no edges)\n",
    "data.num_nodes = 5\n",
    "\n",
    "# Optionally, add node features (here we just use ones)\n",
    "data.x = torch.ones((data.num_nodes, 1))\n",
    "\n",
    "# Compute the Laplacian Positional Encodings\n",
    "data = compute_lap_pe(data, pe_dim=3, use_signnet=False)\n",
    "\n",
    "# Print the computed positional encodings\n",
    "print(\"Laplacian Positional Encodings (lap_pe):\")\n",
    "print(data.lap_pe)\n",
    "\n"
   ],
   "id": "32670c9d11a8d340",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 9\u001B[0m\n\u001B[0;32m      3\u001B[0m edge_index \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\n\u001B[0;32m      4\u001B[0m     [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m],\n\u001B[0;32m      5\u001B[0m     [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      6\u001B[0m ], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Create a Data object\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m data \u001B[38;5;241m=\u001B[39m Data(edge_index\u001B[38;5;241m=\u001B[39medge_index)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Set the number of nodes (important if some nodes have no edges)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m data\u001B[38;5;241m.\u001B[39mnum_nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Data' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: Combine LapPE with SignNet\n",
    "   - Implement SignNet to make the model sign-invariant.\n",
    "   - Use SignNet to process the eigenvectors of the Laplacian to produce consistent embeddings.\n"
   ],
   "id": "784dd4c7fa67e9df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SignNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SignNet: A neural network that is invariant to the sign flips of input eigenvectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if _ == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out_layer = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for SignNet.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input eigenvectors of shape (num_nodes, in_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sign-invariant embeddings of shape (num_nodes, out_dim)\n",
    "        \"\"\"\n",
    "        x_pos = self.mlp(x)\n",
    "        x_neg = self.mlp(-x)\n",
    "        x = x_pos + x_neg\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_lap_pe_with_signnet(data, pe_dim=10, hidden_dim=64, out_dim=32, num_layers=2):\n",
    "    \"\"\"\n",
    "    Compute Laplacian Positional Encodings and process them with SignNet.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): Graph data object.\n",
    "        pe_dim (int): Number of eigenvectors to compute.\n",
    "        hidden_dim (int): Hidden dimension for SignNet.\n",
    "        out_dim (int): Output dimension for SignNet embeddings.\n",
    "        num_layers (int): Number of layers in SignNet.\n",
    "\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): Graph data object with 'sign_inv_lap_pe' attribute added.\n",
    "    \"\"\"\n",
    "    num_nodes = data.num_nodes\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Convert to scipy sparse matrix\n",
    "    from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csgraph\n",
    "    from scipy.sparse.linalg import eigsh\n",
    "    from numpy.linalg import eigh\n",
    "\n",
    "    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).astype(float)\n",
    "\n",
    "    # Compute the Laplacian matrix\n",
    "    laplacian = csgraph.laplacian(adj, normed=False)\n",
    "\n",
    "    # For large sparse matrices, use eigsh\n",
    "    try:\n",
    "        eigvals, eigvecs = eigsh(laplacian, k=pe_dim+1, which='SM')\n",
    "        eigvecs = eigvecs[:, eigvals.argsort()]  # Sort eigenvectors\n",
    "    except RuntimeError:\n",
    "        laplacian_dense = laplacian.toarray()\n",
    "        eigvals, eigvecs = eigh(laplacian_dense)\n",
    "        eigvecs = eigvecs[:, eigvals.argsort()]\n",
    "\n",
    "    # Exclude the first eigenvector (corresponding to the smallest eigenvalue)\n",
    "    pe = eigvecs[:, 1:pe_dim+1]\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    pe_tensor = torch.from_numpy(pe).float()  # Shape: (num_nodes, pe_dim)\n",
    "\n",
    "    # Initialize SignNet\n",
    "    signnet = SignNet(in_dim=pe_dim, hidden_dim=hidden_dim, out_dim=out_dim, num_layers=num_layers)\n",
    "\n",
    "    # Process the positional encodings with SignNet\n",
    "    sign_inv_pe = signnet(pe_tensor)  # Shape: (num_nodes, out_dim)\n",
    "\n",
    "    # Add to data\n",
    "    data.sign_inv_lap_pe = sign_inv_pe\n",
    "\n",
    "    return data\n"
   ],
   "id": "1d17aaae6eb393ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example\n",
   "id": "5bad95fc0e1c553f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a simple undirected graph\n",
    "G = nx.Graph()\n",
    "# Add nodes and edges\n",
    "G.add_edges_from([\n",
    "    (0, 1),\n",
    "    (1, 2),\n",
    "    (2, 3),\n",
    "    (3, 4),\n",
    "    (4, 0),\n",
    "    (1, 3),\n",
    "    (0, 2)\n",
    "])\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(6, 4))\n",
    "pos = nx.spring_layout(G, seed=42)  # For consistent layout\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500)\n",
    "plt.title(\"Sample Graph\")\n",
    "plt.show()\n"
   ],
   "id": "ce89b8c2ab78045c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.328073Z",
     "start_time": "2024-11-24T20:09:14.308968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert NetworkX graph to PyTorch Geometric data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "data = from_networkx(G)\n",
    "# Ensure the number of nodes is set\n",
    "data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "# Optionally, add dummy node features (e.g., ones)\n",
    "data.x = torch.ones((data.num_nodes, 1))\n",
    "\n",
    "# Set parameters\n",
    "pe_dim = 3        # Number of eigenvectors to compute (excluding the first trivial one)\n",
    "hidden_dim = 32    # Hidden dimension for SignNet\n",
    "out_dim = 2        # Output dimension for embeddings (for 2D visualization)\n",
    "num_layers = 2     # Number of layers in SignNet\n",
    "\n",
    "# Compute sign-invariant Laplacian Positional Encodings\n",
    "data = compute_lap_pe_with_signnet(data, pe_dim=pe_dim, hidden_dim=hidden_dim, out_dim=out_dim, num_layers=num_layers)\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = data.sign_inv_lap_pe.detach().numpy()  # Convert to NumPy array for plotting\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c='skyblue', s=200)\n",
    "\n",
    "# Annotate each point with the node index\n",
    "for i, (x, y) in enumerate(embeddings):\n",
    "    plt.text(x, y, str(i), fontsize=12, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Sign-Invariant Laplacian Positional Embeddings\")\n",
    "plt.xlabel(\"Embedding Dimension 1\")\n",
    "plt.ylabel(\"Embedding Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "cef3cdb1205c5fe9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Convert NetworkX graph to PyTorch Geometric data\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_networkx\n\u001B[1;32m----> 4\u001B[0m data \u001B[38;5;241m=\u001B[39m from_networkx(G)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Ensure the number of nodes is set\u001B[39;00m\n\u001B[0;32m      6\u001B[0m data\u001B[38;5;241m.\u001B[39mnum_nodes \u001B[38;5;241m=\u001B[39m G\u001B[38;5;241m.\u001B[39mnumber_of_nodes()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'G' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "80ee0556d4f6e620"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: Implement Random Walk Structural Embeddings (RWSE)\n",
    " - Compute RWSE by counting closed walks of a given length `k`:\n",
    "   - Use the trace of powers of the adjacency matrix: `tr(A^2)`, `tr(A^3)`, ..., `tr(A^k)`.\n",
    "   - Concatenate these trace values for embeddings.\n",
    " - Ensure efficient computation for larger graphs.\n",
    " - Compare RWSE with LapPE in terms of performance and computational cost."
   ],
   "id": "f92cc564062eead0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.377381Z",
     "start_time": "2024-11-24T20:09:14.371768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def compute_rwse(data, K=5):\n",
    "    \"\"\"\n",
    "    Compute Random Walk Structural Embeddings (RWSE) for a given graph data.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): Graph data object.\n",
    "        K (int): Maximum walk length.\n",
    "\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): Graph data object with 'rwse' attribute added.\n",
    "    \"\"\"\n",
    "    num_nodes = data.num_nodes\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Convert to scipy sparse matrix\n",
    "    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).astype(float)\n",
    "    adj = sp.csr_matrix(adj)\n",
    "\n",
    "    # Initialize embeddings tensor\n",
    "    rwse_embeddings = np.zeros((num_nodes, K - 1))\n",
    "\n",
    "    # Start with A_power = adjacency matrix\n",
    "    A_power = adj.copy()\n",
    "\n",
    "    # For walk lengths from 2 to K\n",
    "    for k in range(2, K + 1):\n",
    "        # Multiply to get A^k\n",
    "        A_power = A_power.dot(adj)\n",
    "\n",
    "        # Get the diagonal entries\n",
    "        diag_entries = A_power.diagonal()\n",
    "\n",
    "        # Update embeddings\n",
    "        rwse_embeddings[:, k - 2] = diag_entries\n",
    "\n",
    "    # Optionally apply logarithmic scaling to handle large counts\n",
    "    rwse_embeddings = np.log1p(rwse_embeddings)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    rwse_tensor = torch.from_numpy(rwse_embeddings).float()\n",
    "\n",
    "    # Add to data\n",
    "    data.rwse = rwse_tensor\n",
    "\n",
    "    return data\n"
   ],
   "id": "d8923401b2da2746",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.443606Z",
     "start_time": "2024-11-24T20:09:14.419526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "# Load sample dataset\n",
    "dataset = KarateClub()\n",
    "data = dataset[0]\n",
    "\n",
    "# Compute RWSE embeddings\n",
    "K = 3  # Maximum walk length\n",
    "data = compute_rwse(data, K=K)\n",
    "\n",
    "# Print RWSE embeddings\n",
    "print(\"RWSE Embeddings Shape:\", data.rwse.shape)\n",
    "print(\"RWSE Embeddings:\")\n",
    "print(data.rwse)\n",
    "embeddings = data.rwse.detach().numpy()\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c='skyblue', s=200)\n",
    "\n",
    "# Annotate each point with the node index\n",
    "for i, (x, y) in enumerate(embeddings):\n",
    "    plt.text(x, y, str(i), fontsize=12, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Sign-Invariant Laplacian Positional Embeddings\")\n",
    "plt.xlabel(\"Embedding Dimension 1\")\n",
    "plt.ylabel(\"Embedding Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5f58182c20c1afdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWSE Embeddings Shape: torch.Size([34, 2])\n",
      "RWSE Embeddings:\n",
      "tensor([[2.8332, 3.6109],\n",
      "        [2.3026, 3.2189],\n",
      "        [2.3979, 3.1355],\n",
      "        [1.9459, 3.0445],\n",
      "        [1.3863, 1.6094],\n",
      "        [1.6094, 1.9459],\n",
      "        [1.6094, 1.9459],\n",
      "        [1.6094, 2.5649],\n",
      "        [1.7918, 2.3979],\n",
      "        [1.0986, 0.0000],\n",
      "        [1.3863, 1.6094],\n",
      "        [0.6931, 0.0000],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.7918, 2.5649],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.3863, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.7918, 2.1972],\n",
      "        [1.3863, 1.0986],\n",
      "        [1.3863, 1.0986],\n",
      "        [1.0986, 1.0986],\n",
      "        [1.6094, 1.0986],\n",
      "        [1.3863, 1.0986],\n",
      "        [1.6094, 2.1972],\n",
      "        [1.6094, 1.9459],\n",
      "        [1.9459, 1.9459],\n",
      "        [2.5649, 3.2958],\n",
      "        [2.8904, 3.4340]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mrwse\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Plot the embeddings\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[0;32m     20\u001B[0m plt\u001B[38;5;241m.\u001B[39mscatter(embeddings[:, \u001B[38;5;241m0\u001B[39m], embeddings[:, \u001B[38;5;241m1\u001B[39m], c\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskyblue\u001B[39m\u001B[38;5;124m'\u001B[39m, s\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Annotate each point with the node index\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 4: Implement a Pure Graph Transformer\n",
    " - Treat nodes as tokens and apply a standard transformer model.\n",
    " - Use global attention to aggregate information across all nodes.\n",
    " - Start with node labels as initial embeddings and incorporate LapPE and RWSE.\n",
    " - Implement and test the transformer to evaluate its ability to capture graph structure.\n"
   ],
   "id": "6c951945c862ed0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.503840Z",
     "start_time": "2024-11-24T20:09:14.499811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid  # For Cora dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import math"
   ],
   "id": "245af0b4b449bb2a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.576094Z",
     "start_time": "2024-11-24T20:09:14.568048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GraphTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(GraphTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (N, batch_size, d_model)\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=3, nhead=8, dropout=0.1):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphTransformerEncoderLayer(hidden_dim, nhead, hidden_dim * 4, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (num_nodes, input_dim)\n",
    "        x = self.embedding(x)  # (num_nodes, hidden_dim)\n",
    "        x = x.unsqueeze(1)  # (num_nodes, batch_size=1, hidden_dim)\n",
    "\n",
    "        # Add position embedding if needed (already included via LapPE and RWSE)\n",
    "        # x = x + self.position_embedding\n",
    "\n",
    "        x = x.transpose(0, 1)  # (batch_size=1, num_nodes, hidden_dim)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.transpose(0, 1)  # (num_nodes, batch_size=1, hidden_dim)\n",
    "        x = x.squeeze(1)       # (num_nodes, hidden_dim)\n",
    "\n",
    "        # For node classification: output per node\n",
    "        out = self.classifier(self.dropout(x))  # (num_nodes, num_classes)\n",
    "        return out\n"
   ],
   "id": "bf51df7d8da1b2a6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.613194Z",
     "start_time": "2024-11-24T20:09:14.607687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, optimizer, data, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index) if isinstance(model, GCN) else model(data.x)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    logits = model(data.x)\n",
    "    masks = ['train_mask', 'val_mask', 'test_mask']\n",
    "    accuracies = []\n",
    "    for mask in masks:\n",
    "        mask = getattr(data, mask)\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index) if isinstance(model, GCN) else model(data.x)\n",
    "    masks = ['train_mask', 'val_mask', 'test_mask']\n",
    "    accuracies = []\n",
    "    for mask_name in masks:\n",
    "        mask = getattr(data, mask_name)\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accuracies.append(acc)\n",
    "    return accuracies  # Returns [train_acc, val_acc, test_acc]\n",
    "\n"
   ],
   "id": "20a2eb4869f17ff0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.701254Z",
     "start_time": "2024-11-24T20:09:14.653343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset (e.g., Cora)\n",
    "dataset = Planetoid(root='dataset/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Compute LapPE and RWSE\n",
    "data = compute_lap_pe(data, pe_dim=10)\n",
    "data = compute_rwse(data, K=5)\n",
    "\n",
    "# Concatenate node features\n",
    "data.x = torch.cat([data.x, data.lap_pe, data.rwse], dim=-1)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "num_classes = dataset.num_classes\n",
    "num_layers = 3\n",
    "nhead = 8\n",
    "dropout = 0.1\n",
    "\n",
    "model = GraphTransformer(input_dim, hidden_dim, num_classes, num_layers, nhead, dropout)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, data, optimizer, criterion)\n",
    "    train_acc, val_acc, test_acc = test(model, data)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ],
   "id": "79022b550376b928",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csgraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m data \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Compute LapPE and RWSE\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m data \u001B[38;5;241m=\u001B[39m compute_lap_pe(data, pe_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m compute_rwse(data, K\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Concatenate node features\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 20\u001B[0m, in \u001B[0;36mcompute_lap_pe\u001B[1;34m(data, pe_dim, use_signnet)\u001B[0m\n\u001B[0;32m     17\u001B[0m adj \u001B[38;5;241m=\u001B[39m to_scipy_sparse_matrix(edge_index, num_nodes\u001B[38;5;241m=\u001B[39mnum_nodes)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Compute the Laplacian matrix\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m laplacian \u001B[38;5;241m=\u001B[39m csgraph\u001B[38;5;241m.\u001B[39mlaplacian(adj, normed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# For large sparse matrices, use eigsh\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# Compute the smallest k+1 eigenvalues and eigenvectors\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'csgraph' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    " #  Task 5: Test the Performance of LapPE and RWSE Embeddings\n",
    " - Test LapPE and RWSE embeddings with:\n",
    "   - A Graph Neural Network (GNN) model.\n",
    "   - The pure Graph Transformer implemented above.\n",
    " - Compare results to understand how the embeddings impact performance.\n",
    "\n",
    "## Task 6: Test on Datasets\n",
    " - Use the following datasets for training and evaluation:\n",
    "   1. **Peptides-func**:\n",
    "      - Apply an atom-encoder to represent nodes (atoms in molecules).\n",
    "      - Combine the atom-encoder embeddings with LapPE and RWSE.\n",
    "   2. **Cora**:\n",
    "      - Test embeddings and transformer performance on this graph dataset.\n",
    "\n",
    "### Expected Outputs:\n",
    " - Implementations for LapPE, SignNet, RWSE, and Graph Transformer.\n",
    " - Performance results of GNN and Transformer models using LapPE and RWSE embeddings.\n",
    " - Observations and insights on dataset-specific challenges and results.\n",
    "\n",
    " ### Notes:\n",
    " - Ensure efficiency for matrix operations (e.g., sparse operations for large graphs).\n",
    " - Document any challenges encountered and how they were addressed.\n",
    " - Use modular code to make experiments reproducible and flexible for different datasets.\n",
    "\n"
   ],
   "id": "9dfe299ca8f8dc9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_experiments(data, model_type='gnn'):\n",
    "    results = []\n",
    "    for config in configs:\n",
    "        exp_data = copy.deepcopy(data)\n",
    "\n",
    "        # Compute LapPE with SignNet\n",
    "        if config['use_lappe']:\n",
    "            if config.get('use_signnet', False):\n",
    "                exp_data = compute_lap_pe_with_signnet(exp_data, pe_dim=10, signnet_hidden_dim=64, signnet_out_dim=32, num_layers=2)\n",
    "                lap_pe_dim = exp_data.lap_pe.shape[1]\n",
    "            else:\n",
    "                exp_data = compute_lap_pe(exp_data, pe_dim=10)\n",
    "                lap_pe_dim = exp_data.lap_pe.shape[1]\n",
    "        else:\n",
    "            lap_pe_dim = 0\n",
    "\n",
    "        # Compute RWSE\n",
    "        if config['use_rwse']:\n",
    "            exp_data = compute_rwse(exp_data, K=5)\n",
    "            rwse_dim = exp_data.rwse.shape[1]\n",
    "        else:\n",
    "            rwse_dim = 0\n",
    "\n",
    "        # Combine features\n",
    "        features = [exp_data.x]\n",
    "        if config['use_lappe']:\n",
    "            features.append(exp_data.lap_pe)\n",
    "        if config['use_rwse']:\n",
    "            features.append(exp_data.rwse)\n",
    "        exp_data.x = torch.cat(features, dim=-1)\n",
    "\n",
    "        # Initialize model\n",
    "        input_dim = exp_data.x.shape[1]\n",
    "        hidden_dim = 64\n",
    "        num_classes = dataset.num_classes\n",
    "\n",
    "        if model_type == 'gnn':\n",
    "            model = GCN(input_dim, hidden_dim, num_classes)\n",
    "        elif model_type == 'transformer':\n",
    "            model = GraphTransformer(input_dim, hidden_dim, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type: Choose 'gnn' or 'transformer'.\")\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train and evaluate\n",
    "        for epoch in range(1, 201):\n",
    "            loss = train(model, optimizer, exp_data, criterion)\n",
    "            train_acc, val_acc, test_acc = evaluate(model, exp_data)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"[{config['name']}] Epoch {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f}, \"\n",
    "                      f\"Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            'config': config['name'],\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'test_acc': test_acc,\n",
    "        })\n",
    "\n",
    "    return results\n"
   ],
   "id": "cd0a35cb21a71a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "configs = [\n",
    "    {'name': 'Baseline', 'use_lappe': False, 'use_rwse': False},\n",
    "    {'name': 'LapPE', 'use_lappe': True, 'use_rwse': False},\n",
    "    {'name': 'RWSE', 'use_lappe': False, 'use_rwse': True},\n",
    "    {'name': 'LapPE + RWSE', 'use_lappe': True, 'use_rwse': True},\n",
    "]\n"
   ],
   "id": "999ed0d405a62653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.720891Z",
     "start_time": "2024-11-24T20:09:14.706779Z"
    }
   },
   "cell_type": "code",
   "source": "results_gnn = run_experiments(data, model_type='gnn')\n",
   "id": "f3558d1d99d9a505",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_experiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results_gnn \u001B[38;5;241m=\u001B[39m run_experiments(data, model_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgnn\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'run_experiments' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.744480Z",
     "start_time": "2024-11-24T20:09:14.730418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run experiments with Transformer\n",
    "results_transformer = run_experiments(data, model_type='transformer')"
   ],
   "id": "3f3ec59d4c67ef8b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_experiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Run experiments with Transformer\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m results_transformer \u001B[38;5;241m=\u001B[39m run_experiments(data, model_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransformer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'run_experiments' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implement and Test on the Peptides-func Dataset",
   "id": "c72e3149a8d23dc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:14.776936Z",
     "start_time": "2024-11-24T20:09:14.766538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNGraphLevel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GCNGraphLevel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.pool = global_mean_pool\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = atom_encoder(x)  # Use AtomEncoder\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class GraphTransformerGraphLevel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=3, nhead=8, dropout=0.1):\n",
    "        super(GraphTransformerGraphLevel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphTransformerEncoderLayer(hidden_dim, nhead, hidden_dim * 4, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pool = global_mean_pool\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = atom_encoder(x)  # Use AtomEncoder\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.transpose(0, 1).squeeze(1)\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.classifier(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "def preprocess_dataset(dataset, pe_dim=10, K=5, use_signnet=False):\n",
    "    for data in dataset:\n",
    "        # Compute LapPE\n",
    "        if use_signnet:\n",
    "            data = compute_lap_pe_with_signnet(data, pe_dim=pe_dim)\n",
    "        else:\n",
    "            data = compute_lap_pe(data, pe_dim=pe_dim)\n",
    "\n",
    "        # Compute RWSE\n",
    "        data = compute_rwse(data, K=K)\n",
    "\n",
    "        # Concatenate features\n",
    "        x_list = [atom_encoder(data.x)]\n",
    "        x_list.append(data.lap_pe)\n",
    "        x_list.append(data.rwse)\n",
    "        data.x = torch.cat(x_list, dim=-1)\n",
    "    return dataset\n",
    "\n",
    "def train_graph_level(model, optimizer, loader, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate_graph_level(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y.view(-1)).sum().item()\n",
    "            total += data.num_graphs\n",
    "    return correct / total\n",
    "\n"
   ],
   "id": "a5655221356b517e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:16.537362Z",
     "start_time": "2024-11-24T20:09:14.783448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "# Load the Peptides-func dataset\n",
    "dataset_peptides = PygGraphPropPredDataset(name='ogbg-molpcba')\n",
    "\n",
    "# Split the dataset\n",
    "split_idx = dataset_peptides.get_idx_split()\n",
    "train_dataset = dataset_peptides[split_idx['train']]\n",
    "valid_dataset = dataset_peptides[split_idx['valid']]\n",
    "test_dataset = dataset_peptides[split_idx['test']]\n",
    "\n",
    "atom_encoder = AtomEncoder(emb_dim=64)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "train_dataset = preprocess_dataset(train_dataset, pe_dim=10, K=5, use_signnet=True)\n",
    "valid_dataset = preprocess_dataset(valid_dataset, pe_dim=10, K=5, use_signnet=True)\n",
    "test_dataset = preprocess_dataset(test_dataset, pe_dim=10, K=5, use_signnet=True)\n"
   ],
   "id": "68f979f896a7998f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\ogb\\graphproppred\\dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_lap_pe_with_signnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 22\u001B[0m\n\u001B[0;32m     18\u001B[0m valid_loader \u001B[38;5;241m=\u001B[39m DataLoader(valid_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     19\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 22\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m preprocess_dataset(train_dataset, pe_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, K\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, use_signnet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     23\u001B[0m valid_dataset \u001B[38;5;241m=\u001B[39m preprocess_dataset(valid_dataset, pe_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, K\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, use_signnet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     24\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m preprocess_dataset(test_dataset, pe_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, K\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, use_signnet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[14], line 57\u001B[0m, in \u001B[0;36mpreprocess_dataset\u001B[1;34m(dataset, pe_dim, K, use_signnet)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m# Compute LapPE\u001B[39;00m\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_signnet:\n\u001B[1;32m---> 57\u001B[0m         data \u001B[38;5;241m=\u001B[39m compute_lap_pe_with_signnet(data, pe_dim\u001B[38;5;241m=\u001B[39mpe_dim)\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m         data \u001B[38;5;241m=\u001B[39m compute_lap_pe(data, pe_dim\u001B[38;5;241m=\u001B[39mpe_dim)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'compute_lap_pe_with_signnet' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:09:16.538377Z",
     "start_time": "2024-11-24T20:09:16.538377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "input_dim = train_dataset[0].x.shape[1]\n",
    "hidden_dim = 64\n",
    "num_classes = dataset_peptides.num_tasks  # Adjust as per dataset\n",
    "\n",
    "model = GCNGraphLevel(input_dim, hidden_dim, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_graph_level(model, optimizer, train_loader, criterion)\n",
    "    train_acc = evaluate_graph_level(model, train_loader)\n",
    "    val_acc = evaluate_graph_level(model, valid_loader)\n",
    "    test_acc = evaluate_graph_level(model, test_loader)\n",
    "    print(f\"Epoch {epoch}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"
   ],
   "id": "b8d39108a26a07ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "173705ebe46e0961",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
