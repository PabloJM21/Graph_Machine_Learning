{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9533f0-c800-4daa-b8cb-505ed7f4df2b",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Due:  Tue December 3, 8:00am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44773582-50fe-4d66-ba8e-03a368b350ea",
   "metadata": {},
   "source": [
    "## GPS and Hyperparameters\n",
    "\n",
    "This exercise consists of two parts: first, you are to combine global transformer attention (from the last exercise) with message-passing (from the second exercise). It is completely up to you how you combine those aspects, alternating between the two seems to be one of the best available options though. You may use (pure) message-passing layers from pytorch-geometric for this exercise (but obviously not layers like GPSConv that already combine things - especially since GPSConv differs significantly from the architecture in the GPS paper...).\n",
    "\n",
    "The second part of the exercise is to find a good model (with hyperparameters) for peptides-func. For this task, I want you to use the tool weights&biases (wandb.ai) and their \"sweep\" functionality. You can find example code for this below. Since we do not have access to your wandb accounts, please provide screenshots of your results and verify that these models are indeed good.\n",
    "\n",
    "For the hyperparameter tuning, you must perform this on your hybrid architecture. It might be interesting to see in how far the results (which parameters are important etc) differ between pure transformers, pure message-passing (possibly with VN), and hybrid approaches, although such an evaluation is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33633a0d-3859-4d10-b3f4-d27c390a5e5c",
   "metadata": {},
   "source": [
    "## Hybrid GPS-like architecture"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc8a2d74-2089-47eb-9f61-df4f8c2ee9d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:00.918053Z",
     "start_time": "2024-12-01T12:52:55.009507Z"
    }
   },
   "source": [
    "# your model code goes here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GINELayerWithVN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(GINELayerWithVN, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        self.edge_encoder = torch.nn.Linear(edge_dim, out_channels)\n",
    "        # Remove node_encoder from here\n",
    "        self.virtual_node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_channels, out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.edge_encoder.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "        for m in self.virtual_node_mlp:\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, vn_embed, batch):\n",
    "        # x is already encoded via node_encoder in the main model\n",
    "        x = x.float()  # Ensure x is FloatTensor\n",
    "        edge_attr = edge_attr.float()  # Ensure edge_attr is FloatTensor\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Add virtual node embedding to node features\n",
    "        vn_expanded = vn_embed[batch]\n",
    "        x = x + vn_expanded\n",
    "\n",
    "        # Message Passing\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        # Update node embeddings\n",
    "        out = self.mlp(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # Compute messages\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "# Laplacian Positional Encodings (LapPE)\n",
    "def compute_laplace_pe(data, num_eigenvec=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    num_nodes = A.shape[0]\n",
    "    D = np.diag(np.array(A.sum(axis=1)).flatten())\n",
    "    L = D - A.todense()\n",
    "    L = torch.tensor(L, dtype=torch.float, device=device)\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(L)\n",
    "    except RuntimeError:\n",
    "        eigenvalues, eigenvectors = torch.symeig(L, eigenvectors=True)\n",
    "    available_eigenvec = eigenvectors.shape[1] - 1\n",
    "    actual_num_eigenvec = min(num_eigenvec, available_eigenvec)\n",
    "    eigenvectors = eigenvectors[:, 1:1 + actual_num_eigenvec]\n",
    "    if actual_num_eigenvec < num_eigenvec:\n",
    "        pad_size = num_eigenvec - actual_num_eigenvec\n",
    "        padding = torch.zeros(eigenvectors.shape[0], pad_size, device=device)\n",
    "        eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "    return eigenvectors  # Shape: (num_nodes, num_eigenvec)\n",
    "\n",
    "# Random Walk Structural Embeddings (RWSE)\n",
    "def compute_rwse(data, walk_length=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    A = A.todense()\n",
    "    num_nodes = A.shape[0]\n",
    "    A = torch.tensor(A, dtype=torch.float, device=device)\n",
    "    rw_features = []\n",
    "    A_power = A.clone()\n",
    "    for _ in range(walk_length):\n",
    "        diag = torch.diagonal(A_power)\n",
    "        rw_features.append(diag)\n",
    "        A_power = torch.matmul(A_power, A)\n",
    "    rwse = torch.stack(rw_features, dim=1)  # (num_nodes, walk_length)\n",
    "    return rwse  # Shape: (num_nodes, walk_length)\n",
    "\n",
    "# SignNet to ensure sign invariance\n",
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.phi(x) + self.phi(-x)\n",
    "\n",
    "# Graph Transformer Layer with Masking\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=4, dropout=0.1):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=in_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(out_dim, in_dim)\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # x: (sequence_length, batch_size, embed_dim)\n",
    "        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        linear_output = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = x + linear_output\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Layer that combines Message Passing and Transformer\n",
    "class HybridLayer(nn.Module):\n",
    "    def __init__(self, mp_layer, transformer_layer):\n",
    "        super(HybridLayer, self).__init__()\n",
    "        self.mp_layer = mp_layer\n",
    "        self.transformer_layer = transformer_layer\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, vn_embed, batch):\n",
    "        # Message Passing Layer\n",
    "        x = self.mp_layer(x, edge_index, edge_attr, vn_embed, batch)\n",
    "        x = F.relu(x)\n",
    "        return x  # Return x to update vn_embed before transformer\n",
    "\n",
    "    def apply_transformer(self, x, batch):\n",
    "        # Prepare for Transformer Layer\n",
    "        x_padded, mask = pyg.utils.to_dense_batch(x, batch)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [max_num_nodes, batch_size, hidden_features]\n",
    "        key_padding_mask = ~mask  # [batch_size, max_num_nodes]\n",
    "        x_padded = self.transformer_layer(x_padded, key_padding_mask=key_padding_mask)\n",
    "        x_padded = x_padded.transpose(0, 1)  # x_padded: [batch_size, max_num_nodes, hidden_features]\n",
    "        x = x_padded[mask]  # x: [num_nodes, hidden_features]\n",
    "        return x\n",
    "\n",
    "# Updated GNN Model with Virtual Node, GINE Layers, and Graph Transformer\n",
    "class GNNWithVirtualNodeAndGINEAndTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, edge_attr_dim, num_layers=5, lap_pe_dim=10, rwse_dim=10, num_heads=4):\n",
    "        super(GNNWithVirtualNodeAndGINEAndTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        # Node Encoder\n",
    "        self.node_encoder = nn.Linear(in_features, hidden_features)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mp_layer = GINELayerWithVN(\n",
    "                in_channels=hidden_features,\n",
    "                out_channels=hidden_features,\n",
    "                edge_dim=edge_attr_dim\n",
    "            )\n",
    "            transformer_layer = GraphTransformerLayer(\n",
    "                in_dim=hidden_features,\n",
    "                out_dim=hidden_features,\n",
    "                num_heads=num_heads\n",
    "            )\n",
    "            self.layers.append(HybridLayer(mp_layer, transformer_layer))\n",
    "\n",
    "        self.virtual_node_embedding = torch.nn.Embedding(1, hidden_features)\n",
    "        torch.nn.init.constant_(self.virtual_node_embedding.weight.data, 0)\n",
    "\n",
    "        self.mlp_virtual_node = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_features, hidden_features),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Positional Encodings\n",
    "        self.lap_pe_dim = lap_pe_dim\n",
    "        self.rwse_dim = rwse_dim\n",
    "        self.lap_pe_linear = nn.Linear(hidden_features, hidden_features)\n",
    "        self.rwse_linear = nn.Linear(rwse_dim, hidden_features)\n",
    "        self.signnet = SignNet(lap_pe_dim, hidden_features)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_features, out_features)\n",
    "\n",
    "def forward(self, x, edge_index, edge_attr, batch, data):\n",
    "    # Apply node_encoder first\n",
    "    x = self.node_encoder(x)  # [num_nodes, hidden_features]\n",
    "    # Initialize positional encodings tensor\n",
    "    pos_enc = torch.zeros_like(x).to(device)  # [num_nodes, hidden_features]\n",
    "\n",
    "    # Use data.num_graphs to get the correct number of graphs\n",
    "    batch_size = data.num_graphs\n",
    "\n",
    "    # Initialize virtual node embedding\n",
    "    vn_embed = self.virtual_node_embedding.weight.repeat(batch_size, 1)  # [batch_size, hidden_features]\n",
    "\n",
    "    # Iterate over each graph in the batch\n",
    "    for graph_id in range(batch_size):\n",
    "        mask = (batch == graph_id)\n",
    "        node_idx = mask.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "        # Handle case when graph has no nodes\n",
    "        if node_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract subgraph using pyg.utils.subgraph\n",
    "        sub_edge_index, sub_edge_attr = pyg.utils.subgraph(\n",
    "            node_idx,\n",
    "            edge_index,\n",
    "            edge_attr,\n",
    "            relabel_nodes=True,\n",
    "            num_nodes=x.size(0)\n",
    "        )\n",
    "\n",
    "        # Create sub_data\n",
    "        sub_data = pyg.data.Data(\n",
    "            x=x[node_idx],\n",
    "            edge_index=sub_edge_index,\n",
    "            edge_attr=sub_edge_attr\n",
    "        )\n",
    "\n",
    "        # Compute Positional Encodings for the sub-graph\n",
    "        lap_pe = compute_laplace_pe(sub_data, num_eigenvec=self.lap_pe_dim)\n",
    "        rwse = compute_rwse(sub_data, walk_length=self.rwse_dim)\n",
    "\n",
    "        # Apply SignNet to LapPE\n",
    "        lap_pe = self.signnet(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "        # Linear transformation\n",
    "        lap_pe = self.lap_pe_linear(lap_pe)  # [num_nodes_graph, hidden_features]\n",
    "        rwse = self.rwse_linear(rwse)        # [num_nodes_graph, hidden_features]\n",
    "\n",
    "        # Combine positional encodings\n",
    "        graph_pos_enc = lap_pe + rwse  # [num_nodes_graph, hidden_features]\n",
    "\n",
    "        # Assign to pos_enc\n",
    "        pos_enc[node_idx] = graph_pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "    # Add positional encodings to node features\n",
    "    x = x + pos_enc  # [num_nodes, hidden_features]\n",
    "\n",
    "    for layer in self.layers:\n",
    "        # Message Passing Layer\n",
    "        x = layer(x, edge_index, edge_attr, vn_embed, batch)\n",
    "\n",
    "        # Update virtual node embedding\n",
    "        vn_aggr = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "        vn_embed = vn_embed + self.mlp_virtual_node(vn_aggr)  # [batch_size, hidden_features]\n",
    "\n",
    "        # Transformer Layer\n",
    "        x = layer.apply_transformer(x, batch)\n",
    "\n",
    "    # Apply global mean pooling\n",
    "    x = global_mean_pool(x, batch)  # [batch_size, hidden_features]\n",
    "    x = self.fc(x)  # [batch_size, out_features]\n",
    "    return x\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "625a8487-01eb-46c2-ace6-7211b900b406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:00.922399Z",
     "start_time": "2024-12-01T12:53:00.919067Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e53b0ec0-7936-4b9c-884f-b126418135f5",
   "metadata": {},
   "source": [
    "# WandB hyperparameter tuning example code"
   ]
  },
  {
   "cell_type": "code",
   "id": "10328b2a-634d-4c8e-93a2-ae255ae1b28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:00.926368Z",
     "start_time": "2024-12-01T12:53:00.923497Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch_scatter\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "59a8fe3f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before using wandb, you need to create an account. Then you can login by pasting your API key when prompted. (just the key, nothing else)"
   ]
  },
  {
   "cell_type": "code",
   "id": "aba8bfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:02.886898Z",
     "start_time": "2024-12-01T12:53:00.927375Z"
    }
   },
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: mak84271. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "652f0fe4-eef7-4f8d-9b1a-93f66d72839a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:02.893937Z",
     "start_time": "2024-12-01T12:53:02.888904Z"
    }
   },
   "source": [
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f973d525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.142541Z",
     "start_time": "2024-12-01T12:53:02.894944Z"
    }
   },
   "source": [
    "cora = pyg.datasets.Planetoid(root = \"dataset/cora\", name=\"Cora\")\n",
    "cora_graph = cora[0]\n",
    "cora_dense_adj = pyg.utils.to_dense_adj(cora_graph.edge_index).to(device)\n",
    "# cora_graph.x = cora_graph.x.unsqueeze(0) # Add an empty batch dimension. I needed that for compatibility with MolHIV later.\n",
    "cora_graph = cora_graph.to(device)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fc7ea1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.149438Z",
     "start_time": "2024-12-01T12:53:03.143547Z"
    }
   },
   "source": [
    "cora_graph.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "39becd27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.159325Z",
     "start_time": "2024-12-01T12:53:03.150446Z"
    }
   },
   "source": [
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=torch.nn.functional.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.W: torch.Tensor = torch.nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        torch.nn.init.kaiming_normal_(self.W) \n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = H.clone()\n",
    "        out += torch_scatter.scatter_add(H[edge_index[0]], edge_index[1], dim=0)\n",
    "        out = out.matmul(self.W)\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "baea114a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.170311Z",
     "start_time": "2024-12-01T12:53:03.160336Z"
    }
   },
   "source": [
    "def get_accuracy(model, cora, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cora_graph.x, cora_graph.edge_index)\n",
    "    correct = (outputs[mask].argmax(-1) == cora_graph.y[mask]).sum()\n",
    "    return int(correct) / int(mask.sum())"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "a6b356e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.180817Z",
     "start_time": "2024-12-01T12:53:03.171318Z"
    }
   },
   "source": [
    "class GraphNet(torch.nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, activation=torch.nn.functional.relu, dropout=0.1):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.activation = activation\n",
    "        if dropout>0:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "        else: \n",
    "            self.dropout = torch.nn.Identity()\n",
    "\n",
    "        self.layer_1 = GCNLayer(in_features=in_features, out_features=hidden_features)\n",
    "        self.layer_2 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.layer_3 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.dense1 = torch.nn.Linear(in_features=hidden_features, out_features=hidden_features)\n",
    "        self.dense2 = torch.nn.Linear(in_features=hidden_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = self.layer_1(H, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_2(out, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        H = self.layer_3(out, edge_index)\n",
    "        H = self.dropout(out)\n",
    "        out = self.dense1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "        # H = torch.softmax(H, dim=-1)\n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "38c03c9a",
   "metadata": {},
   "source": [
    "## WandB train function\n",
    "\n",
    "We make a few changes to our train function to enable wandb logging of hyperparameters and metrics. The train function is written to allow both manual runs and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "id": "2071964d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:53:03.191404Z",
     "start_time": "2024-12-01T12:53:03.181826Z"
    }
   },
   "source": [
    "def train(config=None, project=None, notes=None):\n",
    "\n",
    "    with wandb.init(config=config, project=project, notes=notes): # Initialize a new wandb run\n",
    "        # By passing our config through wandb,\n",
    "        # a) it is automatically logged\n",
    "        # b) we can use wandb sweeps to optimize hyperparameters\n",
    "        config = wandb.config \n",
    "\n",
    "        model = GraphNet(\n",
    "            in_features=cora_graph.num_features, \n",
    "            out_features=cora.num_classes, \n",
    "            hidden_features=config.hidden_features, \n",
    "            dropout=config.dropout).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_model = None\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cora_graph.x, cora_graph.edge_index) # we run on everything\n",
    "\n",
    "            loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step() # update parameters\n",
    "            scheduler.step() # update the learning rate once per epoch\n",
    "\n",
    "            val_acc = get_accuracy(model, cora_graph, cora_graph.val_mask)\n",
    "            wandb.log({\"val_acc\": val_acc, \"loss\": loss.item()})\n",
    "\n",
    "            if epoch % 10 == 0 and not wandb.run.sweep_id:\n",
    "                # Only print information on individual runs, not on sweeps\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}, Val accuracy: {val_acc}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model, best_epoch, best_val_acc\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "b173ddcf",
   "metadata": {},
   "source": [
    "## Manual training runs\n",
    "\n",
    "With wandb, you can still manually run your training loop with different hyperparameters as you are used to."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d47a66b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-01T12:53:03.192409Z"
    }
   },
   "source": [
    "best_model, best_model_epoch, best_val_acc = train(dict(\n",
    "    hidden_features=128,\n",
    "    lr=0.01,\n",
    "    dropout=0.1,\n",
    "    epochs=100\n",
    "), project=\"Cora_GraphNet\", notes=\"first trial\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c5b9ca8b008be13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b0a0ad4db19a89dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd346675",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "test_acc = get_accuracy(best_model, cora_graph, cora_graph.test_mask)\n",
    "print(f\"Test acc: {test_acc:.2f} (using model from epoch {best_model_epoch} with val acc {best_val_acc:.2})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49baed50",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "But you can also perform a hyperparameter search using wandb sweeps, by specifying a hyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "id": "be78650c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "sweep_config = {\n",
    "    # hyperparameter search methods, e.g. grid, random\n",
    "    'method': 'random',\n",
    "\n",
    "    # metric to optimize\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "\n",
    "    # parameters to search\n",
    "    'parameters': {\n",
    "        'hidden_features': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'dropout': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.5,\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.001, 0.0001, 0.00001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [100, 200, 300]\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdb6ca89",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Cora_GraphNet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f32d0c7",
   "metadata": {},
   "source": [
    "You can click on the `Sweep URL` to get a nice visualization on how well different sets of hyperparameters perform and to see which are the best (click on the best run and then on Overview).\n",
    "\n",
    "The following cell performs 5 runs using the sweep configuration given above. You can call `wandb.agent` multiple times to produce more runs for the same sweep configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "57e588c3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "wandb.agent(sweep_id, function=train, count=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb03889b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Close the sweep, otherwise individual runs after the sweep will still be logged as part of it\n",
    "wandb.teardown() "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Peptide dataset",
   "id": "39370cb2b090800b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "\n",
    "# Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load the peptides-func dataset\n",
    "# Define a transform function\n",
    "def to_float(data):\n",
    "    data.x = data.x.float()\n",
    "    data.edge_attr = data.edge_attr.float()\n",
    "    return data\n",
    "\n",
    "# Load the dataset with the transform\n",
    "dataset = LRGBDataset(root='dataset/peptides-func', name='Peptides-func', transform=to_float)\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "\n",
    "# Determine the number of node features, edge features, and classes\n",
    "num_node_features = dataset.num_features\n",
    "num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Shuffle the dataset\n",
    "torch.manual_seed(42)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Split the dataset\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_total = len(dataset)\n",
    "num_train = int(num_total * train_ratio)\n",
    "num_val = int(num_total * val_ratio)\n",
    "\n",
    "train_dataset = dataset[:num_train]\n",
    "val_dataset = dataset[num_train:num_train + num_val]\n",
    "test_dataset = dataset[num_train + num_val:]\n",
    "\n",
    "print(f'Train graphs: {len(train_dataset)}')\n",
    "print(f'Validation graphs: {len(val_dataset)}')\n",
    "print(f'Test graphs: {len(test_dataset)}')\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(dataset, batch_size, shuffle):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "# We will create data loaders inside the train function\n",
    "# Your model class\n",
    "GraphNet = GNNWithVirtualNodeAndGINEAndTransformer\n",
    "\n"
   ],
   "id": "cbd5fcc05d90fdbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def train(config=None, project=None, notes=None):\n",
    "    with wandb.init(config=config, project=project, notes=notes):\n",
    "        config = wandb.config\n",
    "\n",
    "        model = GraphNet(\n",
    "            in_features=num_node_features,\n",
    "            hidden_features=config.hidden_features,\n",
    "            out_features=num_classes,\n",
    "            edge_attr_dim=num_edge_features,\n",
    "            num_layers=config.num_layers,\n",
    "            lap_pe_dim=config.lap_pe_dim,\n",
    "            rwse_dim=config.rwse_dim,\n",
    "            num_heads=config.num_heads\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_model = None\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = create_data_loader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = create_data_loader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "        test_loader = create_data_loader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data.x, data.edge_index, data.edge_attr, data.batch, data)\n",
    "                loss = criterion(outputs, data.y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            val_acc = evaluate(model, val_loader)\n",
    "            wandb.log({\"val_acc\": val_acc, \"loss\": total_loss / len(train_loader)})\n",
    "\n",
    "            if epoch % 10 == 0 and not wandb.run.sweep_id:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}, Val accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_acc = evaluate(best_model, test_loader)\n",
    "        wandb.log({\"test_acc\": test_acc})\n",
    "\n",
    "        print(f\"Best Epoch: {best_epoch}, Best Validation Accuracy: {best_val_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        return best_model, best_epoch, best_val_acc, test_acc\n"
   ],
   "id": "b61567a17ff43090",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.edge_attr, data.batch, data)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            print(f\"preds shape: {preds.shape}, data.y shape: {data.y.squeeze().shape}, num_graphs: {data.num_graphs}\")\n",
    "            correct += (preds == data.y.squeeze()).sum().item()\n",
    "            total += data.num_graphs\n",
    "    return correct / total\n"
   ],
   "id": "64b7de26f73826c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'hidden_features': {'values': [64, 128, 256]},\n",
    "        'num_layers': {'values': [3, 5, 7]},\n",
    "        'num_heads': {'values': [4, 8]},\n",
    "        'lap_pe_dim': {'values': [5, 10, 15]},\n",
    "        'rwse_dim': {'values': [5, 10, 15]},\n",
    "        'dropout': {'min': 0.0, 'max': 0.5},\n",
    "        'lr': {'min': 1e-4, 'max': 1e-2, 'distribution': 'log_uniform'},\n",
    "        'weight_decay': {'min': 0.0, 'max': 1e-4},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'value': 100}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='peptides-func-hyperparameter-tuning')\n"
   ],
   "id": "4b47e3c6688bb1cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.login()  # Ensure you are logged in to WandB\n",
    "\n",
    "# Start the sweep agent\n",
    "wandb.agent(sweep_id, function=train)\n"
   ],
   "id": "3c5256d47b2847c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "10150d70055e40b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
