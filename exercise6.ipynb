{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9533f0-c800-4daa-b8cb-505ed7f4df2b",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Due:  Tue December 3, 8:00am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44773582-50fe-4d66-ba8e-03a368b350ea",
   "metadata": {},
   "source": [
    "## GPS and Hyperparameters\n",
    "\n",
    "This exercise consists of two parts: first, you are to combine global transformer attention (from the last exercise) with message-passing (from the second exercise). It is completely up to you how you combine those aspects, alternating between the two seems to be one of the best available options though. You may use (pure) message-passing layers from pytorch-geometric for this exercise (but obviously not layers like GPSConv that already combine things - especially since GPSConv differs significantly from the architecture in the GPS paper...).\n",
    "\n",
    "The second part of the exercise is to find a good model (with hyperparameters) for peptides-func. For this task, I want you to use the tool weights&biases (wandb.ai) and their \"sweep\" functionality. You can find example code for this below. Since we do not have access to your wandb accounts, please provide screenshots of your results and verify that these models are indeed good.\n",
    "\n",
    "For the hyperparameter tuning, you must perform this on your hybrid architecture. It might be interesting to see in how far the results (which parameters are important etc) differ between pure transformers, pure message-passing (possibly with VN), and hybrid approaches, although such an evaluation is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33633a0d-3859-4d10-b3f4-d27c390a5e5c",
   "metadata": {},
   "source": [
    "## Hybrid GPS-like architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a2d74-2089-47eb-9f61-df4f8c2ee9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a8487-01eb-46c2-ace6-7211b900b406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e53b0ec0-7936-4b9c-884f-b126418135f5",
   "metadata": {},
   "source": [
    "# WandB hyperparameter tuning example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10328b2a-634d-4c8e-93a2-ae255ae1b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch_scatter\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8fe3f",
   "metadata": {},
   "source": [
    "Before using wandb, you need to create an account. Then you can login by pasting your API key when prompted. (just the key, nothing else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f0fe4-eef7-4f8d-9b1a-93f66d72839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = pyg.datasets.Planetoid(root = \"dataset/cora\", name=\"Cora\")\n",
    "cora_graph = cora[0]\n",
    "cora_dense_adj = pyg.utils.to_dense_adj(cora_graph.edge_index).to(device)\n",
    "# cora_graph.x = cora_graph.x.unsqueeze(0) # Add an empty batch dimension. I needed that for compatibility with MolHIV later.\n",
    "cora_graph = cora_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ea1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39becd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=torch.nn.functional.relu):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.W: torch.Tensor = torch.nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        torch.nn.init.kaiming_normal_(self.W) \n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = H.clone()\n",
    "        out += torch_scatter.scatter_add(H[edge_index[0]], edge_index[1], dim=0)\n",
    "        out = out.matmul(self.W)\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, cora, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cora_graph.x, cora_graph.edge_index)\n",
    "    correct = (outputs[mask].argmax(-1) == cora_graph.y[mask]).sum()\n",
    "    return int(correct) / int(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(torch.nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, activation=torch.nn.functional.relu, dropout=0.1):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.activation = activation\n",
    "        if dropout>0:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "        else: \n",
    "            self.dropout = torch.nn.Identity()\n",
    "\n",
    "        self.layer_1 = GCNLayer(in_features=in_features, out_features=hidden_features)\n",
    "        self.layer_2 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.layer_3 = GCNLayer(in_features=hidden_features, out_features=hidden_features, activation=self.activation)\n",
    "        self.dense1 = torch.nn.Linear(in_features=hidden_features, out_features=hidden_features)\n",
    "        self.dense2 = torch.nn.Linear(in_features=hidden_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, H: torch.Tensor, edge_index: torch.Tensor):\n",
    "        out = self.layer_1(H, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_2(out, edge_index)\n",
    "        out = self.dropout(out)\n",
    "        H = self.layer_3(out, edge_index)\n",
    "        H = self.dropout(out)\n",
    "        out = self.dense1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "        # H = torch.softmax(H, dim=-1)\n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c03c9a",
   "metadata": {},
   "source": [
    "## WandB train function\n",
    "\n",
    "We make a few changes to our train function to enable wandb logging of hyperparameters and metrics. The train function is written to allow both manual runs and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None, project=None, notes=None):\n",
    "\n",
    "    with wandb.init(config=config, project=project, notes=notes): # Initialize a new wandb run\n",
    "        # By passing our config through wandb,\n",
    "        # a) it is automatically logged\n",
    "        # b) we can use wandb sweeps to optimize hyperparameters\n",
    "        config = wandb.config \n",
    "\n",
    "        model = GraphNet(\n",
    "            in_features=cora_graph.num_features, \n",
    "            out_features=cora.num_classes, \n",
    "            hidden_features=config.hidden_features, \n",
    "            dropout=config.dropout).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=0)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_model = None\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(cora_graph.x, cora_graph.edge_index) # we run on everything\n",
    "\n",
    "            loss = criterion(outputs[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask]) # but only propagate the loss for the train labels\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step() # update parameters\n",
    "            scheduler.step() # update the learning rate once per epoch\n",
    "\n",
    "            val_acc = get_accuracy(model, cora_graph, cora_graph.val_mask)\n",
    "            wandb.log({\"val_acc\": val_acc, \"loss\": loss.item()})\n",
    "\n",
    "            if epoch % 10 == 0 and not wandb.run.sweep_id:\n",
    "                # Only print information on individual runs, not on sweeps\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}, Val accuracy: {val_acc}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model, best_epoch, best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173ddcf",
   "metadata": {},
   "source": [
    "## Manual training runs\n",
    "\n",
    "With wandb, you can still manually run your training loop with different hyperparameters as you are used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_model_epoch, best_val_acc = train(dict(\n",
    "    hidden_features=128,\n",
    "    lr=0.01,\n",
    "    dropout=0.1,\n",
    "    epochs=100\n",
    "), project=\"Cora_GraphNet\", notes=\"first trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd346675",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = get_accuracy(best_model, cora_graph, cora_graph.test_mask)\n",
    "print(f\"Test acc: {test_acc:.2f} (using model from epoch {best_model_epoch} with val acc {best_val_acc:.2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49baed50",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "But you can also perform a hyperparameter search using wandb sweeps, by specifying a hyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    # hyperparameter search methods, e.g. grid, random\n",
    "    'method': 'random',\n",
    "\n",
    "    # metric to optimize\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "\n",
    "    # parameters to search\n",
    "    'parameters': {\n",
    "        'hidden_features': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'dropout': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.5,\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.001, 0.0001, 0.00001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [100, 200, 300]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Cora_GraphNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32d0c7",
   "metadata": {},
   "source": [
    "You can click on the `Sweep URL` to get a nice visualization on how well different sets of hyperparameters perform and to see which are the best (click on the best run and then on Overview).\n",
    "\n",
    "The following cell performs 5 runs using the sweep configuration given above. You can call `wandb.agent` multiple times to produce more runs for the same sweep configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e588c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the sweep, otherwise individual runs after the sweep will still be logged as part of it\n",
    "wandb.teardown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2f61e-6d17-4aaf-8a83-b36a70f39916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
